# # main.py (ä¿®æ”¹å)

# import hydra
# from omegaconf import DictConfig, OmegaConf
# import torch
# # [åˆ é™¤] ä¸å†éœ€è¦ torch.multiprocessing

# import sys
# from pathlib import Path

# sys.path.insert(0, str(Path(__file__).resolve().parent))

# import os
# os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

# from src.data_processing.build_math_datasets import build_datasets
# from src.data_processing.build_enhancement_data import create_enhancement_data
# from evaluate import run_evaluation
# from src.utils.distributed_utils import setup_distributed, cleanup_distributed, is_main_process, get_rank


# def run_distributed_task(config: DictConfig):
#     """
#     [æ–°] ç”¨äºåˆ†å¸ƒå¼ä»»åŠ¡çš„æ‰§è¡Œå‡½æ•°ã€‚
#     å®ƒç”±æ¯ä¸ª torchrun å¯åŠ¨çš„è¿›ç¨‹ç›´æ¥è°ƒç”¨ã€‚
#     """
#     try:
#         # ä»ç¯å¢ƒå˜é‡è·å–rankå’Œworld_sizeï¼Œè¿™æ˜¯torchrunçš„æ ‡å‡†åšæ³•
#         rank = int(os.environ["RANK"])
#         world_size = int(os.environ["WORLD_SIZE"])
        
#         setup_distributed(rank, world_size)

#         # ä½¿ç”¨Hydraçš„instantiateæ¥åŠ¨æ€åˆ›å»ºè®­ç»ƒå™¨
#         trainer = hydra.utils.instantiate(config.trainer, config=config, rank=rank, world_size=world_size)

#         task = config.main.task
#         if task == "train_sft":
#             trainer.train()
#         elif task == "train_rl":
#             trainer.learn()

#     except Exception as e:
#         print(f"FATAL ERROR in worker process rank {get_rank()}: {e}")
#         import traceback
#         traceback.print_exc()
#     finally:
#         cleanup_distributed()


# @hydra.main(version_base=None, config_path="./configs", config_name="lcare_config")
# def main(config: DictConfig) -> None:
#     """
#     [V-FINAL - TORCHRUN COMPATIBLE] é¡¹ç›®ä¸»å…¥å£ã€‚
#     åˆ†å¸ƒå¼ä»»åŠ¡ä¸å†ä½¿ç”¨mp.spawnï¼Œè€Œæ˜¯ç›´æ¥æ‰§è¡Œã€‚
#     """
#     print("=" * 60)
#     print(" L-CARE Project Main Entry (torchrun-compatible) ")
#     print("=" * 60)
#     if is_main_process(): # åªåœ¨ä¸»è¿›ç¨‹æ‰“å°é…ç½®
#         print(" Resolved Configuration:")
#         print(OmegaConf.to_yaml(config))
#         print("-" * 60)

#     task = config.main.task
#     valid_tasks = ["process_data", "train_sft", "train_rl", "evaluate", "create_enhancement_data"]
#     if task not in valid_tasks:
#         raise ValueError(f"Unknown task: '{task}'. Available tasks are: {valid_tasks}")

#     if task == "process_data":
#         print(f"ğŸš€ Starting task: {task}...")
#         build_datasets(config)

#     elif task == "create_enhancement_data":
#         print(f"ğŸš€ Starting task: {task}...")
#         create_enhancement_data(config)

#     elif task in ["train_sft", "train_rl"]:
#         if "RANK" not in os.environ:
#              raise EnvironmentError("RANK env var not found. This script should be launched with `torchrun`.")
#         print(f"ğŸš€ Starting distributed task: {task} on Rank {os.environ['RANK']}...")
#         run_distributed_task(config)

#     elif task == "evaluate":
#         print(f"ğŸš€ Starting task: {task}...")
#         run_evaluation(config)

#     # barrierç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½å®Œæˆåå†æ‰“å°æˆåŠŸä¿¡æ¯
#     if torch.distributed.is_initialized():
#         torch.distributed.barrier()
        
#     if is_main_process():
#         print(f"\nâœ… Task '{task}' finished successfully.")


# if __name__ == "__main__":
#     main()

# main.py (FINAL STABLE & ROBUST)

import hydra
from omegaconf import DictConfig, OmegaConf
import torch

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).resolve().parent))

import os

# å»ºè®®è®¾ç½®é•œåƒæºï¼Œå¦‚æœæ‚¨çš„ç¯å¢ƒéœ€è¦
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

from src.data_processing.build_math_datasets import build_datasets
from src.data_processing.build_enhancement_data import create_enhancement_data
from evaluate import run_evaluation
from src.utils.distributed_utils import setup_distributed, cleanup_distributed, is_main_process, get_rank
from hydra.utils import get_class # å…³é”®ä¿®å¤ï¼šç”¨äºåŠ¨æ€åŠ è½½ç±»
from src.utils.logger import SwanLabLogger


def run_distributed_task(config: DictConfig):
    """
    ç”±æ¯ä¸ª torchrun å¯åŠ¨çš„è¿›ç¨‹ç›´æ¥è°ƒç”¨çš„åˆ†å¸ƒå¼ä»»åŠ¡æ‰§è¡Œå‡½æ•°ã€‚
    """
    try:
        rank = int(os.environ["RANK"])
        world_size = int(os.environ["WORLD_SIZE"])

        # åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
        logger = SwanLabLogger(config, rank)
        
        # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
        setup_distributed(rank, world_size)

        # --- [CRITICAL FIX] æ›¿æ¢ instantiate ä»¥é¿å…å‚æ•°å†²çª ---
        # 1. ä»é…ç½®ä¸­è·å–ç›®æ ‡ç±»çš„å®Œæ•´è·¯å¾„ (e.g., 'src.trainers.sft_trainer.SFTTrainer')
        target_class_path = config.trainer._target_
        
        # 2. ä½¿ç”¨ hydra çš„å·¥å…·å‡½æ•°åŠ¨æ€åœ°åŠ è½½è¿™ä¸ªç±»
        TrainerClass = get_class(target_class_path)

        # 3. ç›´æ¥ã€æ˜ç¡®åœ°å®ä¾‹åŒ–è¿™ä¸ªç±»ï¼Œä¼ å…¥å®ƒéœ€è¦çš„å‚æ•°
        #    è¿™ç§æ–¹å¼æ›´æ¸…æ™°ï¼Œä¸”é¿å…äº†Hydraè‡ªåŠ¨å®ä¾‹åŒ–æ—¶å¯èƒ½å‡ºç°çš„å‚æ•°åå†²çª
        trainer = TrainerClass(config=config, rank=rank, world_size=world_size, logger=logger)
        # --- ä¿®å¤ç»“æŸ ---

        task = config.main.task
        if task == "train_sft":
            trainer.train()
        elif task == "train_rl":
            trainer.learn()

    except Exception as e:
        # åœ¨å‡ºé”™çš„è¿›ç¨‹ä¸Šæ‰“å°è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
        print(f"FATAL ERROR in worker process rank {get_rank()}: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # ç¡®ä¿åœ¨ä»»ä½•æƒ…å†µä¸‹éƒ½æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
        cleanup_distributed()


@hydra.main(version_base=None, config_path="./configs", config_name="lcare_config")
def main(config: DictConfig) -> None:
    """
    [FINAL-TORCHRUN COMPATIBLE] é¡¹ç›®ä¸»å…¥å£ã€‚
    """
    task = config.main.task

    # åªæœ‰ä¸»è¿›ç¨‹æ‰æ‰“å°å†—é•¿çš„é…ç½®ä¿¡æ¯ï¼Œä¿æŒæ—¥å¿—æ¸…æ´
    if is_main_process():
        print("=" * 60)
        print(f" L-CARE Project Main Entry: Running Task -> {task}")
        print("=" * 60)
        # æ‰“å°å®Œæ•´çš„ã€è§£æåçš„é…ç½®
        print(" Resolved Configuration:")
        print(OmegaConf.to_yaml(config))
        print("-" * 60)
        
    valid_tasks = ["process_data", "train_sft", "train_rl", "evaluate", "create_enhancement_data"]
    if task not in valid_tasks:
        raise ValueError(f"Unknown task: '{task}'. Available tasks are: {valid_tasks}")

    # --- ä»»åŠ¡åˆ†æ´¾ ---
    if task == "process_data":
        # æ•°æ®å¤„ç†æ˜¯å•è¿›ç¨‹ä»»åŠ¡
        if is_main_process():
            print(f"ğŸš€ Starting task: {task}...")
            build_datasets(config)

    elif task == "create_enhancement_data":
        # åˆ›å»ºå¢å¼ºæ•°æ®ä¹Ÿæ˜¯å•è¿›ç¨‹ä»»åŠ¡
        if is_main_process():
            print(f"ğŸš€ Starting task: {task}...")
            create_enhancement_data(config)

    elif task in ["train_sft", "train_rl"]:
        # ç¡®ä¿è„šæœ¬æ˜¯é€šè¿‡ torchrun å¯åŠ¨çš„
        if "RANK" not in os.environ:
            raise EnvironmentError("RANK env var not found. This script should be launched with `torchrun` for distributed tasks.")
        
        if is_main_process():
            print(f"ğŸš€ Starting distributed task: {task} on {os.environ['WORLD_SIZE']} GPUs...")
        
        run_distributed_task(config)

    elif task == "evaluate":
        # è¯„ä¼°é€šå¸¸æ˜¯å•å¡ä»»åŠ¡
        if is_main_process():
            print(f"ğŸš€ Starting task: {task}...")
            run_evaluation(config)

    # ç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½å®Œæˆåå†æ‰“å°æˆåŠŸä¿¡æ¯
    if torch.distributed.is_initialized():
        # è¿™ä¸ª barrier ç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½å®Œæˆäº†å®ƒä»¬çš„å·¥ä½œ
        # (ç‰¹åˆ«æ˜¯å¯¹äºåˆ†å¸ƒå¼ä»»åŠ¡) æ‰ç»§ç»­æ‰§è¡Œ
        torch.distributed.barrier()

    if is_main_process():
        print(f"\nâœ… Task '{task}' finished successfully.")


if __name__ == "__main__":
    main()