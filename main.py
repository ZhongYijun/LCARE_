# # main.py (FINAL ROBUST VERSION)
#
# import hydra
# from omegaconf import DictConfig, OmegaConf
# import torch.distributed as dist
#
# import sys
# from pathlib import Path
#
# sys.path.insert(0, str(Path(__file__).resolve().parent))
# import os
#
# os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
#
# from src.data_processing.build_math_datasets import build_datasets
# from src.data_processing.build_enhancement_data import create_enhancement_data
# from evaluate import run_evaluation  # æˆ‘ä»¬å°†ç›´æŽ¥è°ƒç”¨é‡æž„åŽçš„run_evaluation
# from src.utils.distributed_utils import setup_distributed, cleanup_distributed, is_main_process, get_rank
# from hydra.utils import get_class
# from src.utils.logger import SwanLabLogger
#
#
# def run_distributed_training(config: DictConfig):
#     """
#     ç”¨äºŽSFTå’ŒRLè®­ç»ƒçš„åˆ†å¸ƒå¼ä»»åŠ¡æ‰§è¡Œå‡½æ•°ã€‚
#     """
#     logger = None
#
#     try:
#         rank = int(os.environ["RANK"])
#         world_size = int(os.environ["WORLD_SIZE"])
#
#         setup_distributed(rank, world_size)
#         logger = SwanLabLogger(config, rank)
#
#         target_class_path = config.trainer._target_
#         TrainerClass = get_class(target_class_path)
#         trainer = TrainerClass(config=config, rank=rank, world_size=world_size, logger=logger)
#
#         task = config.main.task
#         if task == "train_sft":
#             trainer.train()
#             dist.barrier()
#             if is_main_process():
#                 trainer.save_model()
#             dist.barrier()
#         elif task == "train_rl":
#             trainer.learn()
#
#     except Exception as e:
#         print(f"FATAL ERROR in worker process rank {get_rank()}: {e}")
#         import traceback
#         traceback.print_exc()
#     finally:
#         if logger is not None and is_main_process():
#             logger.finish()
#         cleanup_distributed()
#
#
# # ã€æ ¸å¿ƒä¿®å¤ã€‘ä¸ºè¯„ä¼°ä»»åŠ¡åˆ›å»ºä¸€ä¸ªæ–°çš„åˆ†å¸ƒå¼æ‰§è¡Œå‡½æ•°
# def run_distributed_evaluation(config: DictConfig):
#     """
#     ç”¨äºŽå¹¶è¡ŒåŒ–è¯„ä¼°çš„åˆ†å¸ƒå¼ä»»åŠ¡æ‰§è¡Œå‡½æ•°ã€‚
#     """
#
#     try:
#         rank = int(os.environ["RANK"])
#         world_size = int(os.environ["WORLD_SIZE"])
#
#         setup_distributed(rank, world_size)
#
#         # ç›´æŽ¥è°ƒç”¨é‡æž„åŽçš„ã€èƒ½æ„ŸçŸ¥åˆ†å¸ƒå¼çŽ¯å¢ƒçš„è¯„ä¼°å‡½æ•°
#         run_evaluation(config=config, rank=rank, world_size=world_size)
#
#     except Exception as e:
#         print(f"FATAL ERROR in worker process rank {get_rank()}: {e}")
#         import traceback
#         traceback.print_exc()
#     finally:
#         cleanup_distributed()
#
#
# @hydra.main(version_base=None, config_path="./configs", config_name="lcare_config_final")
# def main(config: DictConfig) -> None:
#     task = config.main.task
#     if is_main_process():
#         print("=" * 60)
#         print(f" L-CARE Project Main Entry: Running Task -> {task}")
#         print("=" * 60)
#         print(" Resolved Configuration:")
#         print(OmegaConf.to_yaml(config))
#         print("-" * 60)
#
#     valid_tasks = ["process_data", "train_sft", "train_rl", "evaluate", "create_enhancement_data"]
#     if task not in valid_tasks:
#         raise ValueError(f"Unknown task: '{task}'. Available tasks are: {valid_tasks}")
#
#     # --- ä»»åŠ¡åˆ†æ´¾ ---
#     if task in ["train_sft", "train_rl"]:
#         if "RANK" not in os.environ:
#             raise EnvironmentError(f"Task '{task}' must be launched with `torchrun`.")
#         if is_main_process(): print(f"ðŸš€ Starting distributed training: {task}...")
#         run_distributed_training(config)
#
#     # ã€æ ¸å¿ƒä¿®å¤ã€‘å°†'evaluate'ä¹Ÿä½œä¸ºåˆ†å¸ƒå¼ä»»åŠ¡å¤„ç†
#     elif task == "evaluate":
#         if "RANK" not in os.environ:
#             raise EnvironmentError(f"Task '{task}' must be launched with `torchrun`.")
#         if is_main_process(): print(f"ðŸš€ Starting distributed evaluation...")
#         run_distributed_evaluation(config)
#
#     elif task == "process_data":
#         if is_main_process():
#             print(f"ðŸš€ Starting task: {task}...")
#             build_datasets(config)
#
#     elif task == "create_enhancement_data":
#         if is_main_process():
#             print(f"ðŸš€ Starting task: {task}...")
#             create_enhancement_data(config)
#
#     if dist.is_initialized():
#         dist.barrier()
#
#     if is_main_process():
#         print(f"\nâœ… Task '{task}' finished successfully.")
#
#
# if __name__ == "__main__":
#     main()

# # main.py (L-CARE V2 - FINAL ROBUST VERSION)
#
# import hydra
# from omegaconf import DictConfig, OmegaConf
# import torch.distributed as dist
# import os
# import sys
# from pathlib import Path
#
# # å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„ï¼Œç¡®ä¿å¯ä»¥æ­£ç¡®å¯¼å…¥srcä¸‹çš„æ¨¡å—
# sys.path.insert(0, str(Path(__file__).resolve().parent))
#
# # å¯¼å…¥é¡¹ç›®æ¨¡å—
# from src.utils.distributed_utils import setup_distributed, cleanup_distributed, is_main_process, get_rank
# from src.utils.logger import SwanLabLogger
# from hydra.utils import get_class
#
# # --- æŒ‰éœ€å¯¼å…¥ä»»åŠ¡å‡½æ•° ---
# from src.data_processing.build_math_datasets import build_datasets
# from src.data_processing.build_enhancement_data import create_enhancement_data
# from evaluate import run_evaluation
#
#
# def run_distributed_task(config: DictConfig):
#     """
#     ç”±æ¯ä¸ª torchrun å¯åŠ¨çš„è¿›ç¨‹ç›´æŽ¥è°ƒç”¨çš„åˆ†å¸ƒå¼ä»»åŠ¡æ‰§è¡Œå‡½æ•°ã€‚
#     æ­¤å‡½æ•°è´Ÿè´£è®¾ç½®åˆ†å¸ƒå¼çŽ¯å¢ƒã€åˆå§‹åŒ–æ—¥å¿—å’Œè®­ç»ƒå™¨ï¼Œå¹¶æ‰§è¡Œè®­ç»ƒæµç¨‹ã€‚
#     """
#     rank = -1  # åˆå§‹åŒ–rankä»¥å¤‡åœ¨é”™è¯¯æ—¥å¿—ä¸­ä½¿ç”¨
#     logger = None
#     try:
#         # ä»ŽçŽ¯å¢ƒå˜é‡èŽ·å–rankå’Œworld_sizeï¼Œè¿™æ˜¯torchrunçš„æ ‡å‡†åšæ³•
#         rank = int(os.environ["RANK"])
#         world_size = int(os.environ["WORLD_SIZE"])
#
#         # åˆå§‹åŒ–åˆ†å¸ƒå¼çŽ¯å¢ƒ
#         setup_distributed(rank, world_size)
#
#         # åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨ (åªæœ‰ä¸»è¿›ç¨‹ä¼šçœŸæ­£å†™å…¥åˆ°SwanLab)
#         logger = SwanLabLogger(config, rank)
#
#         # --- [å…³é”®ä¿®å¤] ä½¿ç”¨ get_class æ˜¾å¼å®žä¾‹åŒ–ï¼Œé¿å…Hydraçš„å‚æ•°å†²çª ---
#         # 1. ä»Žé…ç½®ä¸­èŽ·å–ç›®æ ‡è®­ç»ƒå™¨ç±»çš„å®Œæ•´è·¯å¾„
#         target_class_path = config.trainer._target_
#
#         # 2. ä½¿ç”¨Hydraçš„å·¥å…·å‡½æ•°åŠ¨æ€åœ°åŠ è½½è¿™ä¸ªç±»
#         TrainerClass = get_class(target_class_path)
#
#         # 3. æ˜Žç¡®åœ°å®žä¾‹åŒ–è¿™ä¸ªç±»ï¼Œä¼ å…¥å®ƒéœ€è¦çš„æ‰€æœ‰å‚æ•°
#         trainer = TrainerClass(config=config, rank=rank, world_size=world_size, logger=logger)
#
#         # --- è®­ç»ƒä¸Žä¿å­˜åˆ†ç¦»ï¼Œä¿è¯FSDPçš„ç¨³å¥æ€§ ---
#         task = config.main.task
#         if task == "train_sft":
#             # æ­¥éª¤ 1: æ‰§è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹
#             trainer.train()
#
#             # æ­¥éª¤ 2: è®­ç»ƒå®Œå…¨ç»“æŸåŽï¼Œè¿›è¡Œä¸€æ¬¡åŒæ­¥ï¼Œç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½å®Œæˆäº†è®­ç»ƒ
#             dist.barrier()
#             if is_main_process():
#                 logger.info("SFT training finished. Synchronizing all processes before final save.")
#
#             # æ­¥éª¤ 3: åªæœ‰ä¸»è¿›ç¨‹è´Ÿè´£ä¿å­˜æ¨¡åž‹ï¼Œè¿™æ˜¯æœ€å®‰å…¨çš„æ–¹å¼
#             if is_main_process():
#                 trainer.save_model()
#
#         elif task == "train_rl":
#             # RLçš„learnæ–¹æ³•å†…éƒ¨åŒ…å«äº†è¿­ä»£å’Œä¿å­˜é€»è¾‘ï¼Œç›´æŽ¥è°ƒç”¨å³å¯
#             trainer.learn()
#
#     except Exception as e:
#         # åœ¨å‡ºé”™çš„è¿›ç¨‹ä¸Šæ‰“å°è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
#         print(f"FATAL ERROR in worker process rank {get_rank()}: {e}")
#         import traceback
#         traceback.print_exc()
#     finally:
#         # ç¡®ä¿åœ¨ä»»ä½•æƒ…å†µä¸‹éƒ½æ¸…ç†èµ„æº
#         if logger and is_main_process():
#             logger.finish()
#         cleanup_distributed()
#
#
# @hydra.main(version_base=None, config_path="./configs", config_name="lcare_config")
# def main(config: DictConfig) -> None:
#     """
#     [L-CARE V2] é¡¹ç›®ä¸»å…¥å£ã€‚
#     - ä½¿ç”¨Hydraè¿›è¡Œé…ç½®ç®¡ç†ã€‚
#     - æ™ºèƒ½åˆ†æ´¾åˆ†å¸ƒå¼ä»»åŠ¡å’Œå•è¿›ç¨‹ä»»åŠ¡ã€‚
#     """
#     task = config.main.task
#
#     # åªæœ‰ä¸»è¿›ç¨‹æ‰æ‰“å°å†—é•¿çš„é…ç½®ä¿¡æ¯ï¼Œä¿æŒæ—¥å¿—æ¸…æ´
#     if is_main_process():
#         print("=" * 80)
#         print(f" L-CARE Project - Main Entry Point | Task: '{task}'")
#         print("=" * 80)
#         # ä½¿ç”¨OmegaConf.to_yamlæ‰“å°å®Œæ•´çš„ã€è§£æžåŽçš„é…ç½®ï¼Œæ›´æ˜“è¯»
#         print("--- Resolved Configuration ---")
#         print(OmegaConf.to_yaml(config))
#         print("------------------------------\n")
#
#     valid_tasks = ["process_data", "create_enhancement_data", "train_sft", "train_rl", "evaluate"]
#     if task not in valid_tasks:
#         raise ValueError(f"Unknown task: '{task}'. Available tasks are: {valid_tasks}")
#
#     # --- ä»»åŠ¡åˆ†æ´¾é€»è¾‘ ---
#
#     # åˆ†å¸ƒå¼ä»»åŠ¡: å¿…é¡»ç”± torchrun å¯åŠ¨
#     if task in ["train_sft", "train_rl"]:
#         if "RANK" not in os.environ:
#             raise EnvironmentError(
#                 f"Task '{task}' is a distributed task and must be launched with `torchrun`.\n"
#                 "Example: torchrun --nproc_per_node=2 main.py main.task=train_rl"
#             )
#
#         if is_main_process():
#             print(f"ðŸš€ Starting distributed task: {task} on {os.environ['WORLD_SIZE']} GPUs...")
#
#         run_distributed_task(config)
#
#     # å•è¿›ç¨‹ä»»åŠ¡: åªåœ¨ä¸»è¿›ç¨‹ä¸Šæ‰§è¡Œ
#     elif task == "process_data":
#         if is_main_process():
#             print(f"ðŸš€ Starting single-process task: {task}...")
#             build_datasets(config)
#
#     elif task == "create_enhancement_data":
#         if is_main_process():
#             print(f"ðŸš€ Starting single-process task: {task}...")
#             create_enhancement_data(config)
#
#     elif task == "evaluate":
#         if is_main_process():
#             print(f"ðŸš€ Starting single-process task: {task}...")
#             # [CRITICAL FIX] ä½¿ç”¨æ­£ç¡®çš„å‡½æ•°ç­¾åè°ƒç”¨è¯„ä¼°å‡½æ•°
#             run_evaluation(config)
#
#     # ä½¿ç”¨barrierç¡®ä¿æ‰€æœ‰è¿›ç¨‹ï¼ˆå°¤å…¶æ˜¯åˆ†å¸ƒå¼ä»»åŠ¡ï¼‰éƒ½å®ŒæˆåŽå†æ‰“å°æœ€ç»ˆçš„æˆåŠŸä¿¡æ¯
#     if dist.is_initialized():
#         dist.barrier()
#
#     if is_main_process():
#         print(f"\nâœ… Task '{task}' finished successfully.")
#
#
# if __name__ == "__main__":
#     main()

# main.py
# [L-CARE V15.6-FIXED - FINAL ROBUST & SYNERGISTIC FORM]

import hydra
from omegaconf import DictConfig
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
import logging
import sys
from pathlib import Path

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„ï¼Œç¡®ä¿å¯ä»¥æ­£ç¡®å¯¼å…¥srcä¸‹çš„æ¨¡å—
sys.path.insert(0, str(Path(__file__).resolve().parent))

# Local project imports
from src.utils.config_loader import load_and_resolve_config
from src.utils.distributed_utils import setup_distributed, cleanup_distributed, is_main_process
from src.utils.logger import SwanLabLogger
from src.data_processing import build_math_datasets, build_enhancement_data, generate_rft_data
from src.trainers.sft_trainer import SFTTrainer
from src.trainers.rl_agent import LCARE_Agent
from evaluate import run_evaluation

# Setup logger
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def run_distributed_task(rank: int, world_size: int, config: DictConfig):
    """
    The main entry point for each distributed process.
    """
    try:
        setup_distributed(rank, world_size)

        # Initialize logger for this specific rank
        # SwanLab logger will only activate on rank 0
        swanlab_logger = SwanLabLogger(config, rank)

        task = config.main.task
        logger.info(f"[Rank {rank}] Starting task: {task}")

        if task == "train_sft":
            trainer = SFTTrainer(config, rank, world_size, swanlab_logger)
            trainer.train()
            if is_main_process():
                trainer.save_model()
        elif task == "train_rl":
            agent = LCARE_Agent(config, rank, world_size, swanlab_logger)
            agent.learn()
        else:
            if is_main_process():
                logger.error(f"Unsupported distributed task: {task}")

    except Exception as e:
        logger.error(f"FATAL ERROR in worker process rank {rank}: {e}", exc_info=True)
    finally:
        if dist.is_initialized():
            cleanup_distributed()
        if 'swanlab_logger' in locals() and is_main_process():
            swanlab_logger.finish()


@hydra.main(version_base=None, config_path="configs", config_name="lcare_config")
def main(config: DictConfig) -> None:
    """
    Hydra entry point. Sets up the environment and spawns distributed processes.
    """
    config = load_and_resolve_config(config)

    # --- [CRITICAL FIX for CUDA in subprocesses] ---
    # Set the multiprocessing start method to 'spawn' at the very beginning.
    # This is mandatory for using CUDA in child processes created via multiprocessing.
    # 'spawn' creates a fresh process, avoiding CUDA re-initialization errors that
    # occur with the default 'fork' method.
    try:
        # force=True is important if the context might be set elsewhere
        mp.set_start_method("spawn", force=True)
        logger.info("âœ… Multiprocessing start method successfully set to 'spawn'.")
    except RuntimeError as e:
        # This might happen in some environments (like Jupyter notebooks) if already set.
        logger.info(f"â„¹ï¸ Multiprocessing start method was already set. Info: {e}")
    # --- [END OF FIX] ---

    task = config.main.task
    world_size = torch.cuda.device_count()

    if task in ["train_sft", "train_rl"]:
        if world_size > 0:
            logger.info(f"Spawning {world_size} processes for distributed task: {task}")
            mp.spawn(run_distributed_task, args=(world_size, config), nprocs=world_size, join=True)
        else:
            logger.error("No CUDA devices found for distributed training.")
    elif task == "process_data":
        logger.info("Running single-process task: process_data")
        build_math_datasets.build_datasets(config)
    elif task == "generate_rft_data":
        logger.info("Running single-process task: generate_rft_data")
        generate_rft_data.generate_rft_data(config)
    elif task == "create_enhancement_data":
        logger.info("Running single-process task: create_enhancement_data")
        build_enhancement_data.create_enhancement_data(config)
    elif task == "evaluate":
        logger.info("Running evaluation...")
        # Standalone evaluation can be run distributedly if needed, or single-process.
        # Here we assume a standalone call might want to use all GPUs.
        if world_size > 1:
            mp.spawn(run_distributed_task, args=(world_size, config), nprocs=world_size, join=True)
        else:
            run_evaluation(config)
    else:
        logger.error(f"Unknown task specified in config: {task}")

    if is_main_process():
        logger.info(f"âœ… Task '{task}' finished successfully.")


if __name__ == "__main__":
    main()